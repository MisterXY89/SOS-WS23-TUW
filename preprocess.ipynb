{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we asses what steps are needed to preprocess the data for the training of the SOM. This answers question 4 of the assignment.\n",
    "\n",
    "> Get the data into the form needed for training SOMs. Describe your preprocessing steps (e.g. transcoding, scaling), why you did it and how you did it. Specifically, if your dataset turns out to be extremely large (very high-dimensional and huge number of vectors so that it does not fit into memory for training SOMs) you may choose to apply subsampling for the training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install liac-arff numpy pandas scikit-learn matplotlib seaborn scipy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/phpchCuL5.arff\", 'r') as f:\n",
    "    data, meta = arff.loadarff(io.StringIO(f.read()))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main steps in the preprocessing of the data. First, we need to transform the data into a format that can be used by the SOM. Second, we need to scale the data so that the SOM can be trained properly.\n",
    "\n",
    "We also need two different files:\n",
    "\n",
    "- **Input Vector File (.vec):** \n",
    "    This file contains the actual data that you want to train the SOM with. Each line corresponds to one input vector (i.e., one data point such as the expression level of a protein). The format of each line is generally a list of numeric values separated by spaces or tabs.\n",
    "\n",
    "- **Template Vector File (.tv):** \n",
    "    This file describes the structure of the input vectors. It lists the names of the variables (e.g., protein names) and optionally their type (e.g., numeric, nominal) and other meta-information. It is used to interpret the input vector file correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by normalizing the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.columns[1:78]\n",
    "df_clean = df.dropna(subset=numeric_columns)\n",
    "numeric_df = df_clean[numeric_columns]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df['class'] = df_clean['class'].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting categorical data to numerical data is done by using the `LabelEncoder` from `sklearn.preprocessing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns\n",
    "categorical_columns = ['Genotype', 'Treatment', 'Behavior']\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df_clean[col] = le.fit_transform(df_clean[col])\n",
    "    label_encoders[col] = le  # Storing the label encoder for each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert the DataFrame into the SOMLib vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_content = f'$XDIM {len(df_clean)}\\n$YDIM 1\\n$VEC_DIM {len(df.columns)-1}\\n'\n",
    "for index, row in df_clean.iterrows():\n",
    "    vec_row = ' '.join(map(str, row[1:]))  # Exclude 'MouseID'\n",
    "    vec_content += f'vec_{index} {vec_row}\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `.tv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_content = '$TYPE template\\n$XDIM {}\\n'.format(len(df.columns)-1)\n",
    "for i, col in enumerate(df.columns[1:], start=1):  # Exclude 'MouseID'\n",
    "    col_type = 'NUMERIC' if col in numeric_columns else 'LABEL'\n",
    "    tv_content += f'{col} {col_type}\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we save the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_file_path = 'data/dataset.vec'\n",
    "tv_file_path = 'data/dataset.tv'\n",
    "\n",
    "with open(vec_file_path, 'w') as vec_file:\n",
    "    vec_file.write(vec_content)\n",
    "    \n",
    "with open(tv_file_path, 'w') as tv_file:\n",
    "    tv_file.write(tv_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transcoding: The original dataset was loaded from an `ARFF` file the data was then converted into a pandas DataFrame to facilitate easy manipulation, analysis, and description of the data (see also the `profile.ipynb` notebook). \n",
    "\n",
    "2. Handling Missing Values: Due to the nature of machine learning algorithms requiring complete data for all features, rows with missing values in the numerical protein expression levels were removed. \n",
    "   This step ensures that the SOM algorithm receives consistent and complete data for training.\n",
    "\n",
    "3. Scaling (Normalization): The protein expression levels were normalized to ensure that each protein has equal weight during the SOM training. \n",
    "   Without normalization, proteins with higher absolute values could disproportionately influence the map, leading to biased results. \n",
    "   Normalization was done using the `MinMaxScaler` from the `sklearn.preprocessing` module, which scales each feature to a given range, in this case, `[0, 1]`. This scaling is crucial for algorithms like SOM that are sensitive to the scale of the data.\n",
    "\n",
    "4. Encoding Categorical Data: The categorical columns 'Genotype', 'Treatment', and 'Behavior' were encoded numerically. Since these are binary categories, label encoding can be used, which assigns a unique integer to each category. \n",
    "\n",
    "Lastly, the data was converted into the SOMLib vector format, which is the format required by the SOMToolbox."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
